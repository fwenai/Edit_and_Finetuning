{
    "layers": [
        1
    ],
    "num_steps": 50,
    "lr": 5e-4,
    "weight_decay": 0,
    "kl_factor": 0,
    "norm_constraint": false,
    "rewrite_module_tmp": "model.layers.{}.mlp.down_proj",
    "layer_module_tmp": "model.layers.{}",
    "mlp_module_tmp": "transformer.h.{}.mlp",
    "attn_module_tmp": "transformer.h.{}.self_attn",
    "ln_f_module": "model.norm",
    "lm_head_module": "lm_head"
}